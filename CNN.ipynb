{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 解压数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !cd /home/aistudio/data/data29604 && unzip -q imgs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "# !ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 读取标签，获得类别，创建空目录以便下一步移动数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
      "['p002' 'p002' 'p002' ... 'p081' 'p081' 'p081']\n"
     ]
    }
   ],
   "source": [
    "# 将'p081','p075'两位司机的数据作为验证集，并将其移动到data/valid/文件夹下\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import shutil\r\n",
    "\r\n",
    "root_dir = \"/home/aistudio/\"\r\n",
    "data_dir = \"/home/aistudio/data/data29604/\"\r\n",
    "\r\n",
    "driver_imgs_list_csv = os.path.join(root_dir, \"work/driver_imgs_list.csv\")\r\n",
    "\r\n",
    "if not os.path.exists(root_dir + \"data/eval_set\"):\r\n",
    "    os.mkdir(root_dir + \"data/eval_set\")\r\n",
    "    # for i in range(10):\r\n",
    "        # os.mkdir(root_dir + \"data/eval_set/c%d\"%i)\r\n",
    "\r\n",
    "if not os.path.exists(root_dir + \"data/train_set\"):\r\n",
    "    os.mkdir(root_dir + \"data/train_set\")\r\n",
    "    # for i in range(10):\r\n",
    "    #     os.mkdir(root_dir + \"data/train_set/c%d\"%i)\r\n",
    "\r\n",
    "datafile = pd.read_csv(driver_imgs_list_csv)\r\n",
    "drivers = datafile.subject.values\r\n",
    "all_class_list = datafile.classname.values\r\n",
    "# 去重\r\n",
    "class_list = []\r\n",
    "for id in all_class_list:\r\n",
    "    if id not in class_list:\r\n",
    "        class_list.append(id)\r\n",
    "class_list = class_list[0:10]\r\n",
    "print(class_list)\r\n",
    "print(drivers)\r\n",
    "\r\n",
    "valid_subjects = ['p081','p075']\r\n",
    "train_subjects = list(set(drivers).difference(set(valid_subjects)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 将训练数据和测试数据复制到train_set and eval_set 目录内，并获取相应的标签txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs # codecs专门用作编码转换\r\n",
    "import os\r\n",
    "import random\r\n",
    "import shutil\r\n",
    "from PIL import Image\r\n",
    "all_file_dir = \"/home/aistudio/data/\"\r\n",
    "eval_image_dir = \"/home/aistudio/data/eval_set/\"\r\n",
    "train_image_dir = \"/home/aistudio/data/train_set/\"\r\n",
    "train_file = codecs.open(os.path.join(all_file_dir, \"train_list.txt\"), 'w')\r\n",
    "eval_file = codecs.open(os.path.join(all_file_dir, \"validate_list.txt\"), 'w')\r\n",
    "label_file = codecs.open(os.path.join(all_file_dir, \"label_list.txt\"), 'w')\r\n",
    "for label in class_list:\r\n",
    "    label_file.write(\"{0} {1}\\n\".format(label[1], label))\r\n",
    "\r\n",
    "for valid_subject in valid_subjects:\r\n",
    "    df_valid = datafile[(datafile[\"subject\"]==valid_subject)]\r\n",
    "    for index, row in df_valid.iterrows():\r\n",
    "        subpath_s = row[\"classname\"] + \"/\" + row[\"img\"]\r\n",
    "        subpath_g = row[\"img\"]\r\n",
    "        if os.path.exists(os.path.join(data_dir,\"train\",subpath_s)):\r\n",
    "            shutil.copy(os.path.join(data_dir, \"train\", subpath_s),os.path.join(root_dir, \"data/eval_set\", subpath_g))\r\n",
    "            final_class = int(row[\"classname\"][1])\r\n",
    "            # if final_class > 0 and final_class<5:\r\n",
    "            #     final_class = 1\r\n",
    "            # elif final_class > 4 and final_class != 9:\r\n",
    "            #     final_class = final_class-3\r\n",
    "            # else:\r\n",
    "            #     final_class = 0\r\n",
    "            eval_file.write(\"{0} {1}\\n\".format(os.path.join(eval_image_dir, subpath_g), final_class))\r\n",
    "        else:\r\n",
    "            print(\"cannot copy {} : {}\".format(row[\"subject\"],subpath_s))\r\n",
    "\r\n",
    "for train_subject in train_subjects:\r\n",
    "    df_train = datafile[(datafile[\"subject\"] == train_subject)]\r\n",
    "    for index, row in df_train.iterrows():\r\n",
    "        subpath_s = row[\"classname\"] + \"/\" + row[\"img\"]\r\n",
    "        subpath_g = row[\"img\"]\r\n",
    "        if os.path.exists(os.path.join(data_dir,\"train\",subpath_s)):\r\n",
    "            shutil.copy(os.path.join(data_dir, \"train\", subpath_s),os.path.join(root_dir, \"data/train_set\", subpath_g))\r\n",
    "            final_class = int(row[\"classname\"][1])\r\n",
    "            # if final_class > 0 and final_class<5:\r\n",
    "            #     final_class = 1\r\n",
    "            # elif final_class > 4 and final_class != 9:\r\n",
    "            #     final_class = final_class-3\r\n",
    "            # else:\r\n",
    "            #     final_class = 0\r\n",
    "            train_file.write(\"{0} {1}\\n\".format(os.path.join(train_image_dir, subpath_g), final_class))\r\n",
    "        else:\r\n",
    "            print(\"cannot copy {} : {}\".format(row[\"subject\"],subpath_s))\r\n",
    "\r\n",
    "train_file.close()\r\n",
    "eval_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-15 18:54:14,661] [    INFO] - Installing vgg16_imagenet module\u001b[0m\n",
      "\u001b[32m[2020-06-15 18:54:14,666] [    INFO] - Module vgg16_imagenet already installed in /home/aistudio/.paddlehub/modules/vgg16_imagenet\u001b[0m\n",
      "\u001b[32m[2020-06-15 18:54:15,087] [    INFO] - 32 pretrained paramaters loaded by PaddleHub\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\r\n",
    "\r\n",
    "#创建日志文件，储存lenet训练结果\r\n",
    "# log_writer = LogWriter(\"./log/vgg16\")\r\n",
    "\r\n",
    "module= hub.Module(name=\"vgg16_imagenet\")\r\n",
    "# module = hub.Module(name=\"vgg19_imagenet\")\r\n",
    "# module = hub.Module(name=\"resnet_v2_50_imagenet\")\r\n",
    "# module =hub.Module(name=\"mobilenet_v2_imagenet\")\r\n",
    "\r\n",
    "input_dict, output_dict, program = module.context(trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlehub.dataset.base_cv_dataset import BaseCVDataset\r\n",
    "   \r\n",
    "class DriverDataset(BaseCVDataset):\t\r\n",
    "   def __init__(self):\t\r\n",
    "       # 数据集存放位置\r\n",
    "       self.dataset_dir = \"/home/aistudio/data\"\r\n",
    "       super(DriverDataset, self).__init__(\r\n",
    "           base_path=self.dataset_dir,\r\n",
    "           train_list_file=\"train_list.txt\",\r\n",
    "           validate_list_file=\"validate_list.txt\",\r\n",
    "        #    test_list_file=\"test_list.txt\",\r\n",
    "        #    predict_file=\"predict_list.txt\",\r\n",
    "        #    label_list_file=\"label_list.txt\",\r\n",
    "           label_list=[\"c0\",\"c1\",\"c2\",\"c3\",\"c4\",\"c5\",\"c6\",\"c7\",\"c8\",\"c9\"]\r\n",
    "           )\r\n",
    "dataset = DriverDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-15 18:54:15,138] [    INFO] - Checkpoint dir: ckpt_20200615185415\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "strategy = hub.DefaultFinetuneStrategy(\r\n",
    "    learning_rate=1e-2,\r\n",
    "    optimizer_name=\"adam\",\r\n",
    "    regularization_coeff=0.1)\r\n",
    "\r\n",
    "config = hub.RunConfig(use_cuda=True, use_data_parallel=True, num_epoch=20, batch_size=32, strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-15 18:54:15,144] [    INFO] - Dataset label map = {'c0': 0, 'c1': 1, 'c2': 2, 'c3': 3, 'c4': 4, 'c5': 5, 'c6': 6, 'c7': 7, 'c8': 8, 'c9': 9}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_reader = hub.reader.ImageClassificationReader(\r\n",
    "    image_width=module.get_expected_image_width(),\r\n",
    "    image_height=module.get_expected_image_height(),\r\n",
    "    images_mean=module.get_pretrained_images_mean(),\r\n",
    "    images_std=module.get_pretrained_images_std(),\r\n",
    "    dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\r\n",
    "import os\r\n",
    "\r\n",
    "def init_log_config():\r\n",
    "    \"\"\"\r\n",
    "    初始化日志相关配置\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    global logger\r\n",
    "    logger = logging.getLogger()\r\n",
    "    logger.setLevel(logging.INFO)\r\n",
    "    log_path = os.path.join(os.getcwd(), 'logs')\r\n",
    "    if not os.path.exists(log_path):\r\n",
    "        os.makedirs(log_path)\r\n",
    "    log_name = os.path.join(log_path, 'train.log')\r\n",
    "    sh = logging.StreamHandler()\r\n",
    "    fh = logging.FileHandler(log_name, mode='w')\r\n",
    "    fh.setLevel(logging.DEBUG)\r\n",
    "    formatter = logging.Formatter(\"%(message)s\")\r\n",
    "    fh.setFormatter(formatter)\r\n",
    "    sh.setFormatter(formatter)\r\n",
    "    logger.addHandler(sh)\r\n",
    "    logger.addHandler(fh)\r\n",
    "\r\n",
    "init_log_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-15 18:54:18,072] [    INFO] - Strategy with slanted triangle learning rate, L2 regularization, \u001b[0m\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py:804: UserWarning: There are no operators in the program to be executed. If you pass Program manually, please use fluid.program_guard to ensure the current Program is being used.\n",
      "  warnings.warn(error_info)\n",
      "\u001b[32m[2020-06-15 18:54:20,290] [    INFO] - Try loading checkpoint from ckpt_20200615185415/ckpt.meta\u001b[0m\n",
      "\u001b[32m[2020-06-15 18:54:20,291] [    INFO] - PaddleHub model checkpoint not found, start from scratch...\u001b[0m\n",
      "\u001b[32m[2020-06-15 18:54:20,302] [    INFO] - PaddleHub finetune start\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlehub.common.logger import logger\r\n",
    "from paddlehub.finetune.checkpoint import load_checkpoint, save_checkpoint\r\n",
    "\r\n",
    "feature_map = output_dict[\"feature_map\"]\r\n",
    "feed_list = [input_dict[\"image\"].name]\r\n",
    "from tb_paddle import SummaryWriter\r\n",
    "tb_writer = SummaryWriter(\"./logs/vgg19\")\r\n",
    "class MyClassifierTask(hub.ImageClassifierTask):\r\n",
    "    def _build_env_start_event(self):\r\n",
    "        self.stepmy = 0\r\n",
    "    def _log_interval_event(self, run_states):\r\n",
    "        self.stepmy = self.stepmy+1\r\n",
    "        score, avg_loss, run_speed = self._calculate_metrics(run_states)\r\n",
    "        avg_acc = score['acc']\r\n",
    "        logger.info(\"{}\\t{}\\t{}\\t{}\".format(self.stepmy,avg_acc,avg_loss,run_speed))\r\n",
    "        tb_writer.add_scalar(\r\n",
    "            tag=\"Loss_{}\".format(self.phase),\r\n",
    "            scalar_value=avg_loss,\r\n",
    "            global_step=self._envs['train'].current_step)\r\n",
    "        tb_writer.add_scalar(\r\n",
    "            tag=\"Acc_{}\".format(self.phase),\r\n",
    "            scalar_value=avg_acc,\r\n",
    "            global_step=self._envs['train'].current_step)\r\n",
    "\r\n",
    "\r\n",
    "task = MyClassifierTask(\r\n",
    "    data_reader=data_reader,\r\n",
    "    feed_list=feed_list,\r\n",
    "    feature=feature_map,\r\n",
    "    num_classes=dataset.num_labels,\r\n",
    "    config=config\r\n",
    "    )\r\n",
    "\r\n",
    "task.finetune_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. All changes under this directory will be kept even after reset. Please clean unnecessary files in time to speed up environment loading.\n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !cp /home/aistudio/work/ResNet50_pretrained.tar /home/aistudio/data/ResNet50_pretrained.tar\r\n",
    "# !cd data && tar -xf ResNet50_pretrained.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压数据集\r\n",
    "# import os\r\n",
    "# import zipfile\r\n",
    "# os.chdir('/home/aistudio/data/data29604')\r\n",
    "# extracting = zipfile.ZipFile('imgs.zip')\r\n",
    "# extracting.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# VisualDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import paddle\r\n",
    "# paddle.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\r\n",
    "# from PIL import Image\r\n",
    "# from visualdl import LogWriter\r\n",
    "# import os\r\n",
    "\r\n",
    "# #确保路径为'/home/aistudio'\r\n",
    "# os.chdir('/home/aistudio')\r\n",
    "\r\n",
    "# #创建 LogWriter 对象，将图像数据存放在 `./log/train`路径下\r\n",
    "# from visualdl import LogWriter\r\n",
    "# log_writer = LogWriter(\"./log/train\",sync_cycle=10)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # -*- coding: UTF-8 -*-\r\n",
    "# \"\"\"\r\n",
    "# 训练常用视觉基础网络，用于分类任务\r\n",
    "# 需要将训练图片，类别文件 label_list.txt 放置在同一个文件夹下\r\n",
    "# 程序会先读取 train.txt 文件获取类别数和图片数量\r\n",
    "# \"\"\"\r\n",
    "# from __future__ import absolute_import\r\n",
    "# from __future__ import division\r\n",
    "# from __future__ import print_function\r\n",
    "# import os\r\n",
    "# import numpy as np\r\n",
    "# import time\r\n",
    "# import math\r\n",
    "# import paddle\r\n",
    "# import paddle.fluid as fluid\r\n",
    "# import codecs\r\n",
    "# import logging\r\n",
    "\r\n",
    "# from paddle.fluid.initializer import MSRA\r\n",
    "# from paddle.fluid.initializer import Uniform\r\n",
    "# from paddle.fluid.param_attr import ParamAttr\r\n",
    "# from PIL import Image\r\n",
    "# from PIL import ImageEnhance\r\n",
    "# import paddlehub as hub\r\n",
    "# #使用VisualDL观察Paddle和手写两种模型进行训练对比\r\n",
    "# import paddle.fluid.dygraph as dygraph\r\n",
    "# from visualdl import LogWriter\r\n",
    "\r\n",
    "\r\n",
    "# train_parameters = {\r\n",
    "#     \"input_size\": [3, 224, 224],\r\n",
    "#     \"class_dim\": -1,  # 分类数，会在初始化自定义 reader 的时候获得\r\n",
    "#     \"image_count\": -1,  # 训练图片数量，会在初始化自定义 reader 的时候获得\r\n",
    "#     \"label_dict\": {},\r\n",
    "#     \"data_dir\": \"data\",  # 训练数据存储地址\r\n",
    "#     \"train_file_list\": \"train.txt\",\r\n",
    "#     \"label_file\": \"label_list.txt\",\r\n",
    "#     \"save_freeze_dir\": \"./freeze_model/resnet50\",\r\n",
    "#     \"save_persistable_dir\": \"./persistable_params/50\",\r\n",
    "#     \"continue_train\": True,        # 是否接着上一次保存的参数接着训练，优先级高于预训练模型\r\n",
    "#     \"pretrained\": False,            # 是否使用预训练的模型，对于inceptionv4模型暂无预训练参数\r\n",
    "#     \"pretrained_dir\":\"./data/ResNet50_pretrained\", \r\n",
    "#     \"mode\": \"train\",\r\n",
    "#     \"num_epochs\": 100,\r\n",
    "#     \"train_batch_size\": 96,\r\n",
    "#     \"mean_rgb\": [127.5, 127.5, 127.5],  # 常用图片的三通道均值，通常来说需要先对训练数据做统计，此处仅取中间值\r\n",
    "#     \"use_gpu\": True,\r\n",
    "#     \"image_enhance_strategy\": {  # 图像增强相关策略\r\n",
    "#         \"need_distort\": True,  # 是否启用图像颜色增强\r\n",
    "#         \"need_rotate\": True,   # 是否需要增加随机角度\r\n",
    "#         \"need_crop\": True,      # 是否要增加裁剪\r\n",
    "#         \"need_flip\": True,      # 是否要增加水平随机翻转\r\n",
    "#         \"hue_prob\": 0.5,\r\n",
    "#         \"hue_delta\": 18,\r\n",
    "#         \"contrast_prob\": 0.5,\r\n",
    "#         \"contrast_delta\": 0.5,\r\n",
    "#         \"saturation_prob\": 0.5,\r\n",
    "#         \"saturation_delta\": 0.5,\r\n",
    "#         \"brightness_prob\": 0.5,\r\n",
    "#         \"brightness_delta\": 0.125\r\n",
    "#     },\r\n",
    "#     \"early_stop\": {\r\n",
    "#         \"sample_frequency\": 500,\r\n",
    "#         \"successive_limit\": 5,\r\n",
    "#         \"good_acc1\": 0.92\r\n",
    "#     },\r\n",
    "#     \"rsm_strategy\": {\r\n",
    "#         \"learning_rate\": 0.001,\r\n",
    "#         \"lr_epochs\": [20, 40, 60, 80, 100],\r\n",
    "#         \"lr_decay\": [1, 0.5, 0.25, 0.1, 0.01, 0.002]\r\n",
    "#     },\r\n",
    "#     \"momentum_strategy\": {\r\n",
    "#         \"learning_rate\": 0.002,\r\n",
    "#         \"lr_epochs\": [20, 40, 60, 80, 100],\r\n",
    "#         \"lr_decay\": [1, 0.5, 0.25, 0.1, 0.01, 0.002]\r\n",
    "#     },\r\n",
    "#     \"sgd_strategy\": {\r\n",
    "#         \"learning_rate\": 0.002,\r\n",
    "#         \"lr_epochs\": [20, 40, 60, 80, 100],\r\n",
    "#         \"lr_decay\": [1, 0.5, 0.25, 0.1, 0.01, 0.002]\r\n",
    "#     },\r\n",
    "#     \"adam_strategy\": {\r\n",
    "#         \"learning_rate\": 0.002\r\n",
    "#     }\r\n",
    "# }\r\n",
    "\r\n",
    "# # 定义残差神经网络（ResNet）\r\n",
    "# class ResNet50():\r\n",
    "#     def __init__(self):\r\n",
    "#         pass\r\n",
    "    \r\n",
    "#     def name(self):\r\n",
    "#         \"\"\"\r\n",
    "#         返回网络名字\r\n",
    "#         :return:\r\n",
    "#         \"\"\"\r\n",
    "#         return 'ResNet50'\r\n",
    "\r\n",
    "#     def net(self, input, class_dim=1000):\r\n",
    "#         def conv_bn_layer(input, num_filters, filter_size, stride=1, groups=1, act=None, name=None):\r\n",
    "#             conv = fluid.layers.conv2d(input=input,\r\n",
    "#                                     num_filters=num_filters,\r\n",
    "#                                     filter_size=filter_size,\r\n",
    "#                                     stride=stride,\r\n",
    "#                                     padding=(filter_size - 1) // 2,\r\n",
    "#                                     groups=groups,\r\n",
    "#                                     act=None,\r\n",
    "#                                     param_attr=ParamAttr(name=name + \"_weights\"),\r\n",
    "#                                     bias_attr=False,\r\n",
    "#                                     name=name + '.conv2d.output.1')\r\n",
    "#             if name == \"conv1\":\r\n",
    "#                 bn_name = \"bn_\" + name\r\n",
    "#             else:\r\n",
    "#                 bn_name = \"bn\" + name[3:]\r\n",
    "#             return fluid.layers.batch_norm(input=conv,\r\n",
    "#                                         act=act,\r\n",
    "#                                         name=bn_name + '.output.1',\r\n",
    "#                                         param_attr=ParamAttr(name=bn_name + '_scale'),\r\n",
    "#                                         bias_attr=ParamAttr(bn_name + '_offset'),\r\n",
    "#                                         moving_mean_name=bn_name + '_mean',\r\n",
    "#                                         moving_variance_name=bn_name + '_variance', )\r\n",
    "\r\n",
    "#         def shortcut(input, ch_out, stride, name):\r\n",
    "#             ch_in = input.shape[1]\r\n",
    "#             if ch_in != ch_out or stride != 1:\r\n",
    "#                 return conv_bn_layer(input, ch_out, 1, stride, name=name)\r\n",
    "#             else:\r\n",
    "#                 return input\r\n",
    "\r\n",
    "#         def bottleneck_block(input, num_filters, stride, name):\r\n",
    "#             conv0 = conv_bn_layer(input=input,\r\n",
    "#                                 num_filters=num_filters,\r\n",
    "#                                 filter_size=1,\r\n",
    "#                                 act='relu',\r\n",
    "#                                 name=name + \"_branch2a\")\r\n",
    "#             conv1 = conv_bn_layer(input=conv0,\r\n",
    "#                                 num_filters=num_filters,\r\n",
    "#                                 filter_size=3,\r\n",
    "#                                 stride=stride,\r\n",
    "#                                 act='relu',\r\n",
    "#                                 name=name + \"_branch2b\")\r\n",
    "#             conv2 = conv_bn_layer(input=conv1,\r\n",
    "#                                 num_filters=num_filters * 4,\r\n",
    "#                                 filter_size=1,\r\n",
    "#                                 act=None,\r\n",
    "#                                 name=name + \"_branch2c\")\r\n",
    "\r\n",
    "#             short = shortcut(input, num_filters * 4, stride, name=name + \"_branch1\")\r\n",
    "\r\n",
    "#             return fluid.layers.elementwise_add(x=short, y=conv2, act='relu', name=name + \".add.output.5\")\r\n",
    "\r\n",
    "#         depth = [3, 4, 6, 3]\r\n",
    "#         num_filters = [64, 128, 256, 512]\r\n",
    "\r\n",
    "#         conv = conv_bn_layer(input=input, num_filters=64, filter_size=7, stride=2, act='relu', name=\"conv1\")\r\n",
    "#         conv = fluid.layers.pool2d(input=conv, pool_size=3, pool_stride=2, pool_padding=1, pool_type='max')\r\n",
    "\r\n",
    "#         for block in range(len(depth)):\r\n",
    "#             for i in range(depth[block]):\r\n",
    "#                 conv_name = \"res\" + str(block + 2) + chr(97 + i)\r\n",
    "#                 conv = bottleneck_block(input=conv,\r\n",
    "#                                         num_filters=num_filters[block],\r\n",
    "#                                         stride=2 if i == 0 and block != 0 else 1,\r\n",
    "#                                         name=conv_name)\r\n",
    "\r\n",
    "#         pool = fluid.layers.pool2d(input=conv, pool_size=7, pool_type='avg', global_pooling=True)\r\n",
    "#         # 停止梯度下降\r\n",
    "#         pool.stop_gradient = True\r\n",
    "#         # 这里再重新加载网络的分类器，大小为本项目的分类大小\r\n",
    "#         out = fluid.layers.fc(input=pool, size=class_dim, act='softmax')\r\n",
    "#         return out\r\n",
    "\r\n",
    "\r\n",
    "# class InceptionV4():\r\n",
    "#     def __init__(self):\r\n",
    "#         pass\r\n",
    "    \r\n",
    "#     def name(self):\r\n",
    "#         \"\"\"\r\n",
    "#         返回网络名字\r\n",
    "#         :return:\r\n",
    "#         \"\"\"\r\n",
    "#         return 'InceptionV4'\r\n",
    "\r\n",
    "#     def net(self, input, class_dim=1000):\r\n",
    "#         x = self.inception_stem(input)\r\n",
    "\r\n",
    "#         for i in range(4):\r\n",
    "#             x = self.inceptionA(x, name=str(i + 1))\r\n",
    "#         x = self.reductionA(x)\r\n",
    "\r\n",
    "#         for i in range(7):\r\n",
    "#             x = self.inceptionB(x, name=str(i + 1))\r\n",
    "#         x = self.reductionB(x)\r\n",
    "\r\n",
    "#         for i in range(3):\r\n",
    "#             x = self.inceptionC(x, name=str(i + 1))\r\n",
    "\r\n",
    "#         pool = fluid.layers.pool2d(\r\n",
    "#             input=x, pool_size=8, pool_type='avg', global_pooling=True)\r\n",
    "\r\n",
    "#         drop = fluid.layers.dropout(x=pool, dropout_prob=0.2)\r\n",
    "\r\n",
    "#         stdv = 1.0 / math.sqrt(drop.shape[1] * 1.0)\r\n",
    "#         out = fluid.layers.fc(\r\n",
    "#             input=drop,\r\n",
    "#             size=class_dim,\r\n",
    "#             act='softmax',\r\n",
    "#             param_attr=ParamAttr(\r\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\r\n",
    "#                 name=\"final_fc_weights\"),\r\n",
    "#             bias_attr=ParamAttr(\r\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\r\n",
    "#                 name=\"final_fc_offset\"))\r\n",
    "#         return out\r\n",
    "\r\n",
    "#     def conv_bn_layer(self,\r\n",
    "#                       data,\r\n",
    "#                       num_filters,\r\n",
    "#                       filter_size,\r\n",
    "#                       stride=1,\r\n",
    "#                       padding=0,\r\n",
    "#                       groups=1,\r\n",
    "#                       act='relu',\r\n",
    "#                       name=None):\r\n",
    "#         conv = fluid.layers.conv2d(\r\n",
    "#             input=data,\r\n",
    "#             num_filters=num_filters,\r\n",
    "#             filter_size=filter_size,\r\n",
    "#             stride=stride,\r\n",
    "#             padding=padding,\r\n",
    "#             groups=groups,\r\n",
    "#             act=None,\r\n",
    "#             param_attr=ParamAttr(name=name + \"_weights\"),\r\n",
    "#             bias_attr=False,\r\n",
    "#             name=name)\r\n",
    "#         bn_name = name + \"_bn\"\r\n",
    "#         return fluid.layers.batch_norm(\r\n",
    "#             input=conv,\r\n",
    "#             act=act,\r\n",
    "#             name=bn_name,\r\n",
    "#             param_attr=ParamAttr(name=bn_name + \"_scale\"),\r\n",
    "#             bias_attr=ParamAttr(name=bn_name + \"_offset\"),\r\n",
    "#             moving_mean_name=bn_name + '_mean',\r\n",
    "#             moving_variance_name=bn_name + '_variance')\r\n",
    "\r\n",
    "#     def inception_stem(self, data, name=None):\r\n",
    "#         conv = self.conv_bn_layer(\r\n",
    "#             data, 32, 3, stride=2, act='relu', name=\"conv1_3x3_s2\")\r\n",
    "#         conv = self.conv_bn_layer(conv, 32, 3, act='relu', name=\"conv2_3x3_s1\")\r\n",
    "#         conv = self.conv_bn_layer(\r\n",
    "#             conv, 64, 3, padding=1, act='relu', name=\"conv3_3x3_s1\")\r\n",
    "\r\n",
    "#         pool1 = fluid.layers.pool2d(\r\n",
    "#             input=conv, pool_size=3, pool_stride=2, pool_type='max')\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             conv, 96, 3, stride=2, act='relu', name=\"inception_stem1_3x3_s2\")\r\n",
    "#         concat = fluid.layers.concat([pool1, conv2], axis=1)\r\n",
    "\r\n",
    "#         conv1 = self.conv_bn_layer(\r\n",
    "#             concat, 64, 1, act='relu', name=\"inception_stem2_3x3_reduce\")\r\n",
    "#         conv1 = self.conv_bn_layer(\r\n",
    "#             conv1, 96, 3, act='relu', name=\"inception_stem2_3x3\")\r\n",
    "\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             concat, 64, 1, act='relu', name=\"inception_stem2_1x7_reduce\")\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             conv2,\r\n",
    "#             64, (7, 1),\r\n",
    "#             padding=(3, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_stem2_1x7\")\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             conv2,\r\n",
    "#             64, (1, 7),\r\n",
    "#             padding=(0, 3),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_stem2_7x1\")\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             conv2, 96, 3, act='relu', name=\"inception_stem2_3x3_2\")\r\n",
    "\r\n",
    "#         concat = fluid.layers.concat([conv1, conv2], axis=1)\r\n",
    "\r\n",
    "#         conv1 = self.conv_bn_layer(\r\n",
    "#             concat, 192, 3, stride=2, act='relu', name=\"inception_stem3_3x3_s2\")\r\n",
    "#         pool1 = fluid.layers.pool2d(\r\n",
    "#             input=concat, pool_size=3, pool_stride=2, pool_type='max')\r\n",
    "\r\n",
    "#         concat = fluid.layers.concat([conv1, pool1], axis=1)\r\n",
    "\r\n",
    "#         return concat\r\n",
    "\r\n",
    "#     def inceptionA(self, data, name=None):\r\n",
    "#         pool1 = fluid.layers.pool2d(\r\n",
    "#             input=data, pool_size=3, pool_padding=1, pool_type='avg')\r\n",
    "#         conv1 = self.conv_bn_layer(\r\n",
    "#             pool1, 96, 1, act='relu', name=\"inception_a\" + name + \"_1x1\")\r\n",
    "\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             data, 96, 1, act='relu', name=\"inception_a\" + name + \"_1x1_2\")\r\n",
    "\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             data, 64, 1, act='relu', name=\"inception_a\" + name + \"_3x3_reduce\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3,\r\n",
    "#             96,\r\n",
    "#             3,\r\n",
    "#             padding=1,\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_a\" + name + \"_3x3\")\r\n",
    "\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             data,\r\n",
    "#             64,\r\n",
    "#             1,\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_a\" + name + \"_3x3_2_reduce\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             96,\r\n",
    "#             3,\r\n",
    "#             padding=1,\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_a\" + name + \"_3x3_2\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             96,\r\n",
    "#             3,\r\n",
    "#             padding=1,\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_a\" + name + \"_3x3_3\")\r\n",
    "\r\n",
    "#         concat = fluid.layers.concat([conv1, conv2, conv3, conv4], axis=1)\r\n",
    "\r\n",
    "#         return concat\r\n",
    "\r\n",
    "#     def reductionA(self, data, name=None):\r\n",
    "#         pool1 = fluid.layers.pool2d(\r\n",
    "#             input=data, pool_size=3, pool_stride=2, pool_type='max')\r\n",
    "\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             data, 384, 3, stride=2, act='relu', name=\"reduction_a_3x3\")\r\n",
    "\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             data, 192, 1, act='relu', name=\"reduction_a_3x3_2_reduce\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3, 224, 3, padding=1, act='relu', name=\"reduction_a_3x3_2\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3, 256, 3, stride=2, act='relu', name=\"reduction_a_3x3_3\")\r\n",
    "\r\n",
    "#         concat = fluid.layers.concat([pool1, conv2, conv3], axis=1)\r\n",
    "\r\n",
    "#         return concat\r\n",
    "\r\n",
    "#     def inceptionB(self, data, name=None):\r\n",
    "#         pool1 = fluid.layers.pool2d(\r\n",
    "#             input=data, pool_size=3, pool_padding=1, pool_type='avg')\r\n",
    "#         conv1 = self.conv_bn_layer(\r\n",
    "#             pool1, 128, 1, act='relu', name=\"inception_b\" + name + \"_1x1\")\r\n",
    "\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             data, 384, 1, act='relu', name=\"inception_b\" + name + \"_1x1_2\")\r\n",
    "\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             data, 192, 1, act='relu', name=\"inception_b\" + name + \"_1x7_reduce\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3,\r\n",
    "#             224, (1, 7),\r\n",
    "#             padding=(0, 3),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_b\" + name + \"_1x7\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3,\r\n",
    "#             256, (7, 1),\r\n",
    "#             padding=(3, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_b\" + name + \"_7x1\")\r\n",
    "\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             data,\r\n",
    "#             192,\r\n",
    "#             1,\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_b\" + name + \"_7x1_2_reduce\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             192, (1, 7),\r\n",
    "#             padding=(0, 3),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_b\" + name + \"_1x7_2\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             224, (7, 1),\r\n",
    "#             padding=(3, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_b\" + name + \"_7x1_2\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             224, (1, 7),\r\n",
    "#             padding=(0, 3),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_b\" + name + \"_1x7_3\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             256, (7, 1),\r\n",
    "#             padding=(3, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_b\" + name + \"_7x1_3\")\r\n",
    "\r\n",
    "#         concat = fluid.layers.concat([conv1, conv2, conv3, conv4], axis=1)\r\n",
    "\r\n",
    "#         return concat\r\n",
    "\r\n",
    "#     def reductionB(self, data, name=None):\r\n",
    "#         pool1 = fluid.layers.pool2d(\r\n",
    "#             input=data, pool_size=3, pool_stride=2, pool_type='max')\r\n",
    "\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             data, 192, 1, act='relu', name=\"reduction_b_3x3_reduce\")\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             conv2, 192, 3, stride=2, act='relu', name=\"reduction_b_3x3\")\r\n",
    "\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             data, 256, 1, act='relu', name=\"reduction_b_1x7_reduce\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3,\r\n",
    "#             256, (1, 7),\r\n",
    "#             padding=(0, 3),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"reduction_b_1x7\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3,\r\n",
    "#             320, (7, 1),\r\n",
    "#             padding=(3, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"reduction_b_7x1\")\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             conv3, 320, 3, stride=2, act='relu', name=\"reduction_b_3x3_2\")\r\n",
    "\r\n",
    "#         concat = fluid.layers.concat([pool1, conv2, conv3], axis=1)\r\n",
    "\r\n",
    "#         return concat\r\n",
    "\r\n",
    "#     def inceptionC(self, data, name=None):\r\n",
    "#         pool1 = fluid.layers.pool2d(\r\n",
    "#             input=data, pool_size=3, pool_padding=1, pool_type='avg')\r\n",
    "#         conv1 = self.conv_bn_layer(\r\n",
    "#             pool1, 256, 1, act='relu', name=\"inception_c\" + name + \"_1x1\")\r\n",
    "\r\n",
    "#         conv2 = self.conv_bn_layer(\r\n",
    "#             data, 256, 1, act='relu', name=\"inception_c\" + name + \"_1x1_2\")\r\n",
    "\r\n",
    "#         conv3 = self.conv_bn_layer(\r\n",
    "#             data, 384, 1, act='relu', name=\"inception_c\" + name + \"_1x1_3\")\r\n",
    "#         conv3_1 = self.conv_bn_layer(\r\n",
    "#             conv3,\r\n",
    "#             256, (1, 3),\r\n",
    "#             padding=(0, 1),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_c\" + name + \"_1x3\")\r\n",
    "#         conv3_2 = self.conv_bn_layer(\r\n",
    "#             conv3,\r\n",
    "#             256, (3, 1),\r\n",
    "#             padding=(1, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_c\" + name + \"_3x1\")\r\n",
    "\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             data, 384, 1, act='relu', name=\"inception_c\" + name + \"_1x1_4\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             448, (1, 3),\r\n",
    "#             padding=(0, 1),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_c\" + name + \"_1x3_2\")\r\n",
    "#         conv4 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             512, (3, 1),\r\n",
    "#             padding=(1, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_c\" + name + \"_3x1_2\")\r\n",
    "#         conv4_1 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             256, (1, 3),\r\n",
    "#             padding=(0, 1),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_c\" + name + \"_1x3_3\")\r\n",
    "#         conv4_2 = self.conv_bn_layer(\r\n",
    "#             conv4,\r\n",
    "#             256, (3, 1),\r\n",
    "#             padding=(1, 0),\r\n",
    "#             act='relu',\r\n",
    "#             name=\"inception_c\" + name + \"_3x1_3\")\r\n",
    "\r\n",
    "#         concat = fluid.layers.concat(\r\n",
    "#             [conv1, conv2, conv3_1, conv3_2, conv4_1, conv4_2], axis=1)\r\n",
    "\r\n",
    "#         return concat\r\n",
    "\r\n",
    "\r\n",
    "# def init_log_config():\r\n",
    "#     \"\"\"\r\n",
    "#     初始化日志相关配置\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     global logger\r\n",
    "#     logger = logging.getLogger()\r\n",
    "#     logger.setLevel(logging.INFO)\r\n",
    "#     log_path = os.path.join(os.getcwd(), 'logs')\r\n",
    "#     if not os.path.exists(log_path):\r\n",
    "#         os.makedirs(log_path)\r\n",
    "#     log_name = os.path.join(log_path, 'train.log')\r\n",
    "#     sh = logging.StreamHandler()\r\n",
    "#     fh = logging.FileHandler(log_name, mode='w')\r\n",
    "#     fh.setLevel(logging.DEBUG)\r\n",
    "#     formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\r\n",
    "#     fh.setFormatter(formatter)\r\n",
    "#     sh.setFormatter(formatter)\r\n",
    "#     logger.addHandler(sh)\r\n",
    "#     logger.addHandler(fh)\r\n",
    "\r\n",
    "\r\n",
    "# def init_train_parameters():\r\n",
    "#     \"\"\"\r\n",
    "#     初始化训练参数，主要是初始化图片数量，类别数\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     train_file_list = os.path.join(train_parameters['data_dir'], train_parameters['train_file_list'])\r\n",
    "#     label_list = os.path.join(train_parameters['data_dir'], train_parameters['label_file'])\r\n",
    "#     index = 0\r\n",
    "#     with codecs.open(label_list, encoding='utf-8') as flist:\r\n",
    "#         lines = [line.strip() for line in flist]\r\n",
    "#         for line in lines:\r\n",
    "#             parts = line.strip().split()\r\n",
    "#             train_parameters['label_dict'][parts[1]] = int(parts[0])\r\n",
    "#             index += 1\r\n",
    "#         train_parameters['class_dim'] = index\r\n",
    "#     with codecs.open(train_file_list, encoding='utf-8') as flist:\r\n",
    "#         lines = [line.strip() for line in flist]\r\n",
    "#         train_parameters['image_count'] = len(lines)\r\n",
    "\r\n",
    "\r\n",
    "# def resize_img(img, target_size):\r\n",
    "#     \"\"\"\r\n",
    "#     强制缩放图片\r\n",
    "#     :param img:\r\n",
    "#     :param target_size:\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     target_size = input_size\r\n",
    "#     img = img.resize((target_size[1], target_size[2]), Image.BILINEAR)\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def random_crop(img, scale=[0.08, 1.0], ratio=[3. / 4., 4. / 3.]):\r\n",
    "#     aspect_ratio = math.sqrt(np.random.uniform(*ratio))\r\n",
    "#     w = 1. * aspect_ratio\r\n",
    "#     h = 1. / aspect_ratio\r\n",
    "\r\n",
    "#     bound = min((float(img.size[0]) / img.size[1]) / (w**2),\r\n",
    "#                 (float(img.size[1]) / img.size[0]) / (h**2))\r\n",
    "#     scale_max = min(scale[1], bound)\r\n",
    "#     scale_min = min(scale[0], bound)\r\n",
    "\r\n",
    "#     target_area = img.size[0] * img.size[1] * np.random.uniform(scale_min,\r\n",
    "#                                                                 scale_max)\r\n",
    "#     target_size = math.sqrt(target_area)\r\n",
    "#     w = int(target_size * w)\r\n",
    "#     h = int(target_size * h)\r\n",
    "\r\n",
    "#     i = np.random.randint(0, img.size[0] - w + 1)\r\n",
    "#     j = np.random.randint(0, img.size[1] - h + 1)\r\n",
    "\r\n",
    "#     img = img.crop((i, j, i + w, j + h))\r\n",
    "#     img = img.resize((train_parameters['input_size'][1], train_parameters['input_size'][2]), Image.BILINEAR)\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def rotate_image(img):\r\n",
    "#     \"\"\"\r\n",
    "#     图像增强，增加随机旋转角度\r\n",
    "#     \"\"\"\r\n",
    "#     angle = np.random.randint(-14, 15)\r\n",
    "#     img = img.rotate(angle)\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def random_brightness(img):\r\n",
    "#     \"\"\"\r\n",
    "#     图像增强，亮度调整\r\n",
    "#     :param img:\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     prob = np.random.uniform(0, 1)\r\n",
    "#     if prob < train_parameters['image_enhance_strategy']['brightness_prob']:\r\n",
    "#         brightness_delta = train_parameters['image_enhance_strategy']['brightness_delta']\r\n",
    "#         delta = np.random.uniform(-brightness_delta, brightness_delta) + 1\r\n",
    "#         img = ImageEnhance.Brightness(img).enhance(delta)\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def random_contrast(img):\r\n",
    "#     \"\"\"\r\n",
    "#     图像增强，对比度调整\r\n",
    "#     :param img:\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     prob = np.random.uniform(0, 1)\r\n",
    "#     if prob < train_parameters['image_enhance_strategy']['contrast_prob']:\r\n",
    "#         contrast_delta = train_parameters['image_enhance_strategy']['contrast_delta']\r\n",
    "#         delta = np.random.uniform(-contrast_delta, contrast_delta) + 1\r\n",
    "#         img = ImageEnhance.Contrast(img).enhance(delta)\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def random_saturation(img):\r\n",
    "#     \"\"\"\r\n",
    "#     图像增强，饱和度调整\r\n",
    "#     :param img:\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     prob = np.random.uniform(0, 1)\r\n",
    "#     if prob < train_parameters['image_enhance_strategy']['saturation_prob']:\r\n",
    "#         saturation_delta = train_parameters['image_enhance_strategy']['saturation_delta']\r\n",
    "#         delta = np.random.uniform(-saturation_delta, saturation_delta) + 1\r\n",
    "#         img = ImageEnhance.Color(img).enhance(delta)\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def random_hue(img):\r\n",
    "#     \"\"\"\r\n",
    "#     图像增强，色度调整\r\n",
    "#     :param img:\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     prob = np.random.uniform(0, 1)\r\n",
    "#     if prob < train_parameters['image_enhance_strategy']['hue_prob']:\r\n",
    "#         hue_delta = train_parameters['image_enhance_strategy']['hue_delta']\r\n",
    "#         delta = np.random.uniform(-hue_delta, hue_delta)\r\n",
    "#         img_hsv = np.array(img.convert('HSV'))\r\n",
    "#         img_hsv[:, :, 0] = img_hsv[:, :, 0] + delta\r\n",
    "#         img = Image.fromarray(img_hsv, mode='HSV').convert('RGB')\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def distort_color(img):\r\n",
    "#     \"\"\"\r\n",
    "#     概率的图像增强\r\n",
    "#     :param img:\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     prob = np.random.uniform(0, 1)\r\n",
    "#     # Apply different distort order\r\n",
    "#     if prob < 0.35:\r\n",
    "#         img = random_brightness(img)\r\n",
    "#         img = random_contrast(img)\r\n",
    "#         img = random_saturation(img)\r\n",
    "#         img = random_hue(img)\r\n",
    "#     elif prob < 0.7:\r\n",
    "#         img = random_brightness(img)\r\n",
    "#         img = random_saturation(img)\r\n",
    "#         img = random_hue(img)\r\n",
    "#         img = random_contrast(img)\r\n",
    "#     return img\r\n",
    "\r\n",
    "\r\n",
    "# def custom_image_reader(file_list, data_dir, mode):\r\n",
    "#     \"\"\"\r\n",
    "#     自定义用户图片读取器，先初始化图片种类，数量\r\n",
    "#     :param file_list:\r\n",
    "#     :param data_dir:\r\n",
    "#     :param mode:\r\n",
    "#     :return:\r\n",
    "#     \"\"\"\r\n",
    "#     with codecs.open(file_list) as flist:\r\n",
    "#         lines = [line.strip() for line in flist]\r\n",
    "\r\n",
    "#     def reader():\r\n",
    "#         np.random.shuffle(lines)\r\n",
    "#         for line in lines:\r\n",
    "#             if mode == 'train' or mode == 'val':\r\n",
    "#                 img_path, label = line.split()\r\n",
    "#                 img = Image.open(img_path)\r\n",
    "#                 try:\r\n",
    "#                     if img.mode != 'RGB':\r\n",
    "#                         img = img.convert('RGB')\r\n",
    "#                     if train_parameters['image_enhance_strategy']['need_distort'] == True:\r\n",
    "#                         img = distort_color(img)\r\n",
    "#                     if train_parameters['image_enhance_strategy']['need_rotate'] == True:\r\n",
    "#                         img = rotate_image(img)\r\n",
    "#                     if train_parameters['image_enhance_strategy']['need_crop'] == True:\r\n",
    "#                         img = random_crop(img, train_parameters['input_size'])\r\n",
    "#                     if train_parameters['image_enhance_strategy']['need_flip'] == True:\r\n",
    "#                         mirror = int(np.random.uniform(0, 2))\r\n",
    "#                         if mirror == 1:\r\n",
    "#                             img = img.transpose(Image.FLIP_LEFT_RIGHT)\r\n",
    "#                     # HWC--->CHW && normalized\r\n",
    "#                     img = np.array(img).astype('float32')\r\n",
    "#                     img -= train_parameters['mean_rgb']\r\n",
    "#                     img = img.transpose((2, 0, 1))  # HWC to CHW\r\n",
    "#                     img *= 0.007843                 # 像素值归一化\r\n",
    "#                     yield img, int(label)\r\n",
    "#                 except Exception as e:\r\n",
    "#                     pass                            # 以防某些图片读取处理出错，加异常处理\r\n",
    "#             elif mode == 'test':\r\n",
    "#                 img_path = os.path.join(data_dir, line)\r\n",
    "#                 img = Image.open(img_path)\r\n",
    "#                 if img.mode != 'RGB':\r\n",
    "#                     img = img.convert('RGB')\r\n",
    "#                 img = resize_img(img, train_parameters['input_size'])\r\n",
    "#                 # HWC--->CHW && normalized\r\n",
    "#                 img = np.array(img).astype('float32')\r\n",
    "#                 img -= train_parameters['mean_rgb']\r\n",
    "#                 img = img.transpose((2, 0, 1))  # HWC to CHW\r\n",
    "#                 img *= 0.007843  # 像素值归一化\r\n",
    "#                 yield img\r\n",
    "\r\n",
    "#     return reader\r\n",
    "\r\n",
    "\r\n",
    "# def optimizer_momentum_setting():\r\n",
    "#     \"\"\"\r\n",
    "#     阶梯型的学习率适合比较大规模的训练数据\r\n",
    "#     \"\"\"\r\n",
    "#     learning_strategy = train_parameters['momentum_strategy']\r\n",
    "#     batch_size = train_parameters[\"train_batch_size\"]\r\n",
    "#     iters = train_parameters[\"image_count\"] // batch_size\r\n",
    "#     lr = learning_strategy['learning_rate']\r\n",
    "\r\n",
    "#     boundaries = [i * iters for i in learning_strategy[\"lr_epochs\"]]\r\n",
    "#     values = [i * lr for i in learning_strategy[\"lr_decay\"]]\r\n",
    "#     learning_rate = fluid.layers.piecewise_decay(boundaries, values)\r\n",
    "#     optimizer = fluid.optimizer.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\r\n",
    "#     return optimizer\r\n",
    "\r\n",
    "\r\n",
    "# def optimizer_rms_setting():\r\n",
    "#     \"\"\"\r\n",
    "#     阶梯型的学习率适合比较大规模的训练数据\r\n",
    "#     \"\"\"\r\n",
    "#     batch_size = train_parameters[\"train_batch_size\"]\r\n",
    "#     iters = train_parameters[\"image_count\"] // batch_size\r\n",
    "#     learning_strategy = train_parameters['rsm_strategy']\r\n",
    "#     lr = learning_strategy['learning_rate']\r\n",
    "\r\n",
    "#     boundaries = [i * iters for i in learning_strategy[\"lr_epochs\"]]\r\n",
    "#     values = [i * lr for i in learning_strategy[\"lr_decay\"]]\r\n",
    "\r\n",
    "#     optimizer = fluid.optimizer.RMSProp(\r\n",
    "#         learning_rate=fluid.layers.piecewise_decay(boundaries, values))\r\n",
    "\r\n",
    "#     return optimizer\r\n",
    "\r\n",
    "\r\n",
    "# def optimizer_sgd_setting():\r\n",
    "#     \"\"\"\r\n",
    "#     loss下降相对较慢，但是最终效果不错，阶梯型的学习率适合比较大规模的训练数据\r\n",
    "#     \"\"\"\r\n",
    "#     learning_strategy = train_parameters['sgd_strategy']\r\n",
    "#     batch_size = train_parameters[\"train_batch_size\"]\r\n",
    "#     iters = train_parameters[\"image_count\"] // batch_size\r\n",
    "#     lr = learning_strategy['learning_rate']\r\n",
    "\r\n",
    "#     boundaries = [i * iters for i in learning_strategy[\"lr_epochs\"]]\r\n",
    "#     values = [i * lr for i in learning_strategy[\"lr_decay\"]]\r\n",
    "#     learning_rate = fluid.layers.piecewise_decay(boundaries, values)\r\n",
    "#     optimizer = fluid.optimizer.SGD(learning_rate=learning_rate)\r\n",
    "#     return optimizer\r\n",
    "\r\n",
    "\r\n",
    "# def optimizer_adam_setting():\r\n",
    "#     \"\"\"\r\n",
    "#     能够比较快速的降低 loss，但是相对后期乏力\r\n",
    "#     \"\"\"\r\n",
    "#     learning_strategy = train_parameters['adam_strategy']\r\n",
    "#     learning_rate = learning_strategy['learning_rate']\r\n",
    "#     optimizer = fluid.optimizer.Adam(learning_rate=learning_rate)\r\n",
    "#     return optimizer\r\n",
    "\r\n",
    "\r\n",
    "# def load_params(exe, program):\r\n",
    "#     if train_parameters['continue_train'] and os.path.exists(train_parameters['save_persistable_dir']):\r\n",
    "#         logger.info('load params from retrain model')\r\n",
    "#         fluid.io.load_persistables(executor=exe,\r\n",
    "#                                    dirname=train_parameters['save_persistable_dir'],\r\n",
    "#                                    main_program=program)\r\n",
    "#     elif train_parameters['pretrained'] and os.path.exists(train_parameters['pretrained_dir']):\r\n",
    "#         logger.info('load params from pretrained model')\r\n",
    "#         def if_exist(var):\r\n",
    "#             return os.path.exists(os.path.join(train_parameters['pretrained_dir'], var.name))\r\n",
    "\r\n",
    "#         fluid.io.load_vars(exe, train_parameters['pretrained_dir'], main_program=program,\r\n",
    "#                            predicate=if_exist)\r\n",
    "\r\n",
    "\r\n",
    "# def train():\r\n",
    "\r\n",
    "\r\n",
    "#     EPOCH_NUM = train_parameters[\"num_epochs\"]\r\n",
    "#     #定义分别存储Paddle和python模型的训练信息的多维数组, 先存Paddle编写的模型的信息，后存python编写模型的信息\r\n",
    "#     train_loss_acc = np.empty([2,EPOCH_NUM], dtype = float) #用二维数组分别存储两种模型的train_loss、test_loss\r\n",
    "#     #定义VisualDL使用的变量。train和test集的loss定义为scalar类型（折线图），weight定义为histogram类型（直方图）\r\n",
    "#     # log_writter = LogWriter(\"./log\", sync_cycle=1000) #定义日志写入器。参数为日志存储位置和写入硬盘的数据量间隔\r\n",
    "#     # with log_writter.mode(\"train\") as logger: #变量定义在logger下操作。设置日志写入模式为“train”\r\n",
    "#     #     train_loss_paddle = logger.scalar(\"Train Loss\") #记录paddle模型train集loss\r\n",
    "#     #     train_acc_paddle = logger.scalar(\"Train Acc\") #记录paddle模型train集loss\r\n",
    "\r\n",
    "#     train_prog = fluid.Program()\r\n",
    "#     train_startup = fluid.Program()\r\n",
    "#     logger.info(\"create prog success\")\r\n",
    "#     logger.info(\"train config: %s\", str(train_parameters))\r\n",
    "#     logger.info(\"build input custom reader and data feeder\")\r\n",
    "#     file_list = os.path.join(train_parameters['data_dir'], \"train.txt\")\r\n",
    "#     mode = train_parameters['mode']\r\n",
    "#     batch_reader = paddle.batch(custom_image_reader(file_list, train_parameters['data_dir'], mode),\r\n",
    "#                                 batch_size=train_parameters['train_batch_size'],\r\n",
    "#                                 drop_last=True)\r\n",
    "#     place = fluid.CUDAPlace(0) if train_parameters['use_gpu'] else fluid.CPUPlace()\r\n",
    "#     # 定义输入数据的占位符\r\n",
    "#     img = fluid.layers.data(name='img', shape=train_parameters['input_size'], dtype='float32')\r\n",
    "#     label = fluid.layers.data(name='label', shape=[1], dtype='int64')\r\n",
    "#     feeder = fluid.DataFeeder(feed_list=[img, label], place=place)\r\n",
    "\r\n",
    "#     # 选取不同的网络\r\n",
    "#     logger.info(\"build newwork\")\r\n",
    "#     # model = InceptionV4()\r\n",
    "#     model = ResNet50()\r\n",
    "#     out = model.net(input=img, class_dim=train_parameters['class_dim'])\r\n",
    "#     cost = fluid.layers.cross_entropy(out, label)\r\n",
    "#     avg_cost = fluid.layers.mean(x=cost)\r\n",
    "#     acc_top1 = fluid.layers.accuracy(input=out, label=label, k=1)\r\n",
    "#     # 选取不同的优化器\r\n",
    "#     # optimizer = optimizer_rms_setting()\r\n",
    "#     # optimizer = optimizer_momentum_setting()\r\n",
    "#     # optimizer = optimizer_sgd_setting()\r\n",
    "#     optimizer = optimizer_adam_setting()\r\n",
    "#     optimizer.minimize(avg_cost)\r\n",
    "#     exe = fluid.Executor(place)\r\n",
    "\r\n",
    "#     main_program = fluid.default_main_program()\r\n",
    "#     exe.run(fluid.default_startup_program())\r\n",
    "#     train_fetch_list = [avg_cost.name, acc_top1.name, out.name]\r\n",
    "    \r\n",
    "#     load_params(exe, main_program)\r\n",
    "\r\n",
    "#     # 训练循环主体\r\n",
    "#     stop_strategy = train_parameters['early_stop']\r\n",
    "#     successive_limit = stop_strategy['successive_limit']\r\n",
    "#     sample_freq = stop_strategy['sample_frequency']\r\n",
    "#     good_acc1 = stop_strategy['good_acc1']\r\n",
    "#     successive_count = 0\r\n",
    "#     stop_train = False\r\n",
    "#     total_batch_count = 0\r\n",
    "#     for pass_id in range(train_parameters[\"num_epochs\"]):\r\n",
    "#         logger.info(\"current pass: %d, start read image\", pass_id)\r\n",
    "#         batch_id = 0\r\n",
    "\r\n",
    "#         for step_id, data in enumerate(batch_reader()):\r\n",
    "#             t1 = time.time()\r\n",
    "#             # logger.info(\"data size:{0}\".format(len(data)))\r\n",
    "#             loss, acc1, pred_ot = exe.run(main_program,\r\n",
    "#                                           feed=feeder.feed(data),\r\n",
    "#                                           fetch_list=train_fetch_list)\r\n",
    "#             t2 = time.time()\r\n",
    "#             batch_id += 1\r\n",
    "#             total_batch_count += 1\r\n",
    "#             period = t2 - t1\r\n",
    "#             loss = np.mean(np.array(loss))\r\n",
    "#             acc1 = np.mean(np.array(acc1))\r\n",
    "#             if batch_id % 100 == 0:\r\n",
    "#                 logger.info(\"Pass {0}, trainbatch {1}, loss {2}, acc1 {3}, time {4}\".format(pass_id, batch_id, loss, acc1,\r\n",
    "#                                                                                             \"%2.2f sec\" % period))\r\n",
    "#                 # print(\"Pass {0}, trainbatch {1}, loss {2}, acc1 {3}, time {4}\".format(pass_id, batch_id, loss, acc1,\r\n",
    "#                                                                                             # \"%2.2f sec\" % period))\r\n",
    "#             # 简单的提前停止策略，认为连续达到某个准确率就可以停止了\r\n",
    "#             if acc1 >= good_acc1:\r\n",
    "#                 successive_count += 1\r\n",
    "#                 logger.info(\"current acc1 {0} meets good {1}, successive count {2}\".format(acc1, good_acc1, successive_count))\r\n",
    "#                 fluid.io.save_inference_model(dirname=train_parameters['save_freeze_dir'],\r\n",
    "#                                               feeded_var_names=['img'],\r\n",
    "#                                               target_vars=[out],\r\n",
    "#                                               main_program=main_program,\r\n",
    "#                                               executor=exe)\r\n",
    "#                 if successive_count >= successive_limit:\r\n",
    "#                     logger.info(\"end training\")\r\n",
    "#                     stop_train = True\r\n",
    "#                     break\r\n",
    "#             else:\r\n",
    "#                 successive_count = 0\r\n",
    "\r\n",
    "#             # 通用的保存策略，减小意外停止的损失\r\n",
    "#             if total_batch_count % sample_freq == 0:\r\n",
    "#                 logger.info(\"temp save {0} batch train result, current acc1 {1}\".format(total_batch_count, acc1))\r\n",
    "#                 fluid.io.save_persistables(dirname=train_parameters['save_persistable_dir'],\r\n",
    "#                                            main_program=main_program,\r\n",
    "#                                            executor=exe)\r\n",
    "#         train_loss_acc[0][pass_id] = loss\r\n",
    "#         train_loss_acc[1][pass_id] = acc1\r\n",
    "#         #  #写入Paddle模型的train和test的loss以及模型的权重weight到log文件夹下的对应文件中\r\n",
    "#         # train_loss_paddle.add_record(pass_id, loss.numpy()[0])\r\n",
    "#         # train_acc_paddle.add_record(pass_id, acc1.numpy()[0])\r\n",
    "#         if stop_train:\r\n",
    "#             break\r\n",
    "#     logger.info(\"training till last epcho, end training\")\r\n",
    "#     fluid.io.save_persistables(dirname=train_parameters['save_persistable_dir'],\r\n",
    "#                                            main_program=main_program,\r\n",
    "#                                            executor=exe)\r\n",
    "#     fluid.io.save_inference_model(dirname=train_parameters['save_freeze_dir'],\r\n",
    "#                                               feeded_var_names=['img'],\r\n",
    "#                                               target_vars=[out],\r\n",
    "#                                               main_program=main_program.clone(for_test=True),\r\n",
    "#                                               executor=exe)\r\n",
    "# init_log_config()\r\n",
    "# init_train_parameters()\r\n",
    "# train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import   \r\n",
    "# from __future__ import division   \r\n",
    "# from __future__ import print_function   \r\n",
    "   \r\n",
    "# import os   \r\n",
    "# import numpy as np   \r\n",
    "# import random   \r\n",
    "# import time   \r\n",
    "# import codecs   \r\n",
    "# import sys   \r\n",
    "# import functools   \r\n",
    "# import math   \r\n",
    "# import paddle   \r\n",
    "# import paddle.fluid as fluid   \r\n",
    "# from paddle.fluid import core   \r\n",
    "# from paddle.fluid.param_attr import ParamAttr   \r\n",
    "# from PIL import Image, ImageEnhance   \r\n",
    "   \r\n",
    "# target_size = [3, 224, 224]   \r\n",
    "# mean_rgb = [127.5, 127.5, 127.5]   \r\n",
    "# data_dir = \"data/\"   \r\n",
    "# eval_file = \"eval.txt\"   \r\n",
    "# use_gpu = True   \r\n",
    "# place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()   \r\n",
    "# exe = fluid.Executor(place)   \r\n",
    "# save_freeze_dir = \"./freeze-model\"   \r\n",
    "# [inference_program, feed_target_names, fetch_targets] = fluid.io.load_inference_model(dirname=save_freeze_dir, executor=exe)   \r\n",
    "# # print(fetch_targets)   \r\n",
    "   \r\n",
    "   \r\n",
    "# def crop_image(img, target_size):   \r\n",
    "#     width, height = img.size   \r\n",
    "#     w_start = (width - target_size[2]) / 2   \r\n",
    "#     h_start = (height - target_size[1]) / 2   \r\n",
    "#     w_end = w_start + target_size[2]   \r\n",
    "#     h_end = h_start + target_size[1]   \r\n",
    "#     img = img.crop((w_start, h_start, w_end, h_end))   \r\n",
    "#     return img   \r\n",
    "   \r\n",
    "   \r\n",
    "# def resize_img(img, target_size):   \r\n",
    "#     ret = img.resize((target_size[1], target_size[2]), Image.BILINEAR)   \r\n",
    "#     return ret   \r\n",
    "   \r\n",
    "   \r\n",
    "# def read_image(img_path):   \r\n",
    "#     img = Image.open(img_path)   \r\n",
    "#     if img.mode != 'RGB':   \r\n",
    "#         img = img.convert('RGB')   \r\n",
    "#     img = crop_image(img, target_size)   \r\n",
    "#     img = np.array(img).astype('float32')   \r\n",
    "#     img -= mean_rgb   \r\n",
    "#     img = img.transpose((2, 0, 1))  # HWC to CHW   \r\n",
    "#     img *= 0.007843   \r\n",
    "#     img = img[np.newaxis,:]   \r\n",
    "#     return img   \r\n",
    "   \r\n",
    "   \r\n",
    "# def infer(image_path):   \r\n",
    "#     tensor_img = read_image(image_path)   \r\n",
    "#     label = exe.run(inference_program, feed={feed_target_names[0]: tensor_img}, fetch_list=fetch_targets)   \r\n",
    "#     return np.argmax(label)   \r\n",
    "   \r\n",
    "   \r\n",
    "# def eval_all():   \r\n",
    "#     eval_file_path = os.path.join(data_dir, eval_file)   \r\n",
    "#     total_count = 0   \r\n",
    "#     right_count = 0   \r\n",
    "#     with codecs.open(eval_file_path, encoding='utf-8') as flist:    \r\n",
    "#         lines = [line.strip() for line in flist]   \r\n",
    "#         t1 = time.time()   \r\n",
    "#         for line in lines:   \r\n",
    "#             total_count += 1   \r\n",
    "#             parts = line.strip().split()   \r\n",
    "#             result = infer(parts[0])   \r\n",
    "#             # print(\"infer result:{0} answer:{1}\".format(result, parts[1]))   \r\n",
    "#             if str(result) == parts[1]:   \r\n",
    "#                 right_count += 1   \r\n",
    "#         period = time.time() - t1   \r\n",
    "#         print(\"total eval count:{0} cost time:{1} predict accuracy:{2}\".format(total_count, \"%2.2f sec\" % period, right_count / total_count))   \r\n",
    "   \r\n",
    "   \r\n",
    "# if 1:   \r\n",
    "#     eval_all() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.7.1 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
